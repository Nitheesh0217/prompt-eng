{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain-of-Thought Prompting\n",
    "\n",
    "Chain-of-Thought (CoT) prompting enhances complex reasoning by encouraging the model to break down problems into intermediate reasoning steps. When combined with few-shot prompting, it can significantly improve performance on tasks that require multi-step reasoning before arriving at a response.\n",
    "\n",
    "## Automatic Chain-of-Thought (Auto-CoT)\n",
    "\n",
    "Traditionally, using CoT prompting with demonstrations involves manually crafting diverse and effective examples. This manual effort is time-consuming and can lead to less-than-optimal results. To address this, Zhang et al. (2022) introduced Auto-CoT, an automated approach that minimizes manual involvement. Their method uses the prompt “Let’s think step by step” to generate reasoning chains automatically for demonstrations. However, this automatic process is not immune to errors. To reduce the impact of such mistakes, the approach emphasizes the importance of diverse demonstrations.\n",
    "\n",
    "Auto-CoT operates in two main stages:\n",
    "\n",
    "1. **Question Clustering:** Questions from the dataset are grouped into clusters based on similarity or relevance.\n",
    "2. **Demonstration Sampling:** A representative question from each cluster is selected, and its reasoning chain is generated using Zero-Shot-CoT guided by simple heuristics.\n",
    "\n",
    "\n",
    "## References:\n",
    "\n",
    "* (Wei et al. (2022),)[https://arxiv.org/abs/2201.11903]\n",
    "* (OpenAI Documentation for Prompt Engineering)[https://platform.openai.com/docs/guides/prompt-engineering]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running this code on MyBind.org\n",
    "\n",
    "Note: remember that you will need to **adjust CONFIG** with **proper URL and API_KEY**!\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/GenILab-FAU/prompt-eng/HEAD?urlpath=%2Fdoc%2Ftree%2Fprompt-eng%2Fchain_of_thought.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid format specifier ' \"Bearer YOUR_API_KEY\"' for object of type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 43\u001b[0m\n\u001b[0;32m     14\u001b[0m     few_shot_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(reader)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m5\u001b[39m]  \u001b[38;5;66;03m# Assuming the response is in the sixth column\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 2) Define a more detailed Chain-of-Thought prompt\u001b[39;00m\n\u001b[0;32m     17\u001b[0m COT_PROMPT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124m# **Data & Methodologies**\u001b[39m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;124m## **1. Overview**\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124mThe data and methodologies employed in this project are crucial to ensuring the integrity, accuracy, and reliability of the results. This section details the sources of data, the methods of acquisition, preprocessing steps, and the analytical techniques used to extract meaningful insights. The structured approach guarantees data consistency and reproducibility.\u001b[39m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;124m---\u001b[39m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[38;5;124m## **2. Data Collection**\u001b[39m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[38;5;124m### **2.1 Data Sources**\u001b[39m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124mThe data used in this project is aggregated from multiple sources to ensure comprehensive coverage and reliability. The primary data sources include:\u001b[39m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124m- **Public APIs**: Extracting real-time data from social media platforms such as Twitter and Reddit.\u001b[39m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124m- **Web Scraping**: Collecting user-generated content from forums and news websites.\u001b[39m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124m- **Database Queries**: Retrieving historical data from structured databases using SQL.\u001b[39m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;124m### **2.2 Data Acquisition Methods**\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124mThe following techniques were employed for data collection:\u001b[39m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124m- **API Calls**: Utilized REST APIs to fetch structured data in JSON format.\u001b[39m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124m- **Web Scraping**: Implemented with `BeautifulSoup` and `Scrapy` to extract unstructured text data.\u001b[39m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124m- **SQL Queries**: Used for efficient retrieval and filtering of historical data from relational databases.\u001b[39m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;124m#### **Example API Request Code:**\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124m```python\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124mimport requests\u001b[39m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124murl = \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.example.com/data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;124mheaders = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer YOUR_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124mresponse = requests.get(url, headers=headers)\u001b[39m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124mdata = response.json()\u001b[39m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124m```\u001b[39m\n\u001b[0;32m     47\u001b[0m \n\u001b[0;32m     48\u001b[0m \u001b[38;5;124m---\u001b[39m\n\u001b[0;32m     49\u001b[0m \n\u001b[0;32m     50\u001b[0m \u001b[38;5;124m## **3. Data Preprocessing**\u001b[39m\n\u001b[0;32m     51\u001b[0m \n\u001b[0;32m     52\u001b[0m \u001b[38;5;124m### **3.1 Data Cleaning**\u001b[39m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124mTo ensure high data quality, the following preprocessing steps were applied:\u001b[39m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124m- **Handling Missing Values**: Missing numerical values were imputed using the median, and categorical values were replaced with the mode.\u001b[39m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124m- **Data Normalization**: Feature scaling was applied using StandardScaler to ensure uniformity across datasets.\u001b[39m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124m- **Feature Engineering**: New derived features were created to enhance model performance.\u001b[39m\n\u001b[0;32m     57\u001b[0m \n\u001b[0;32m     58\u001b[0m \u001b[38;5;124m#### **Example Preprocessing Code:**\u001b[39m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124m```python\u001b[39m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124mimport pandas as pd\u001b[39m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124mfrom sklearn.preprocessing import StandardScaler\u001b[39m\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m \u001b[38;5;124mdf = pd.read_csv(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124mdf.fillna(df.median(), inplace=True)\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124mscaler = StandardScaler()\u001b[39m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124mdf[[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature2\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]] = scaler.fit_transform(df[[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature2\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]])\u001b[39m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124m```\u001b[39m\n\u001b[0;32m     68\u001b[0m \n\u001b[0;32m     69\u001b[0m \u001b[38;5;124m---\u001b[39m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m \u001b[38;5;124m## **4. Analytical Techniques**\u001b[39m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;124m### **4.1 Statistical Methods**\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124m- **Descriptive Statistics**: Used for summarizing trends and distributions within the dataset.\u001b[39m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124m- **Correlation Analysis**: Measured relationships between different variables to identify key influencing factors.\u001b[39m\n\u001b[0;32m     76\u001b[0m \n\u001b[0;32m     77\u001b[0m \u001b[38;5;124m### **4.2 Machine Learning Models**\u001b[39m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124m- **Random Forest Classifier**: Used for sentiment classification due to its robustness in handling imbalanced data.\u001b[39m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124m- **K-Means Clustering**: Applied to segment users based on engagement patterns.\u001b[39m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124m- **Neural Networks**: Implemented for advanced predictive modeling and sentiment analysis.\u001b[39m\n\u001b[0;32m     81\u001b[0m \n\u001b[0;32m     82\u001b[0m \u001b[38;5;124m#### **Example Model Training Code:**\u001b[39m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124m```python\u001b[39m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124mfrom sklearn.ensemble import RandomForestClassifier\u001b[39m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124mfrom sklearn.model_selection import train_test_split\u001b[39m\n\u001b[0;32m     86\u001b[0m \n\u001b[0;32m     87\u001b[0m \u001b[38;5;124mX = df.drop(columns=[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m])\u001b[39m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124my = df[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124mX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\u001b[39m\n\u001b[0;32m     90\u001b[0m \n\u001b[0;32m     91\u001b[0m \u001b[38;5;124mmodel = RandomForestClassifier(n_estimators=100)\u001b[39m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124mmodel.fit(X_train, y_train)\u001b[39m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;124mpredictions = model.predict(X_test)\u001b[39m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;124m```\u001b[39m\n\u001b[0;32m     95\u001b[0m \n\u001b[0;32m     96\u001b[0m \u001b[38;5;124m---\u001b[39m\n\u001b[0;32m     97\u001b[0m \n\u001b[0;32m     98\u001b[0m \u001b[38;5;124m## **5. Conclusion**\u001b[39m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124mThe data collection, preprocessing, and analytical methods detailed above ensure a rigorous approach to extracting insights. The methodologies implemented maintain high standards of accuracy, consistency, and reproducibility, supporting the reliability of the project outcomes.\u001b[39m\n\u001b[0;32m    100\u001b[0m \n\u001b[0;32m    101\u001b[0m \n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[38;5;124m[START OF INSTRUCTIONS]\u001b[39m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mfew_shot_output\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124m[END OF INSTRUCTIONS]\u001b[39m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# 3) Create a payload with moderate token usage\u001b[39;00m\n\u001b[0;32m    109\u001b[0m payload_cot \u001b[38;5;241m=\u001b[39m create_payload(\n\u001b[0;32m    110\u001b[0m     target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopen-webui\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    111\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlama-3.2-3B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Updated model\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m     num_predict\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m                 \u001b[38;5;66;03m# Enough tokens for detailed paragraphs\u001b[39;00m\n\u001b[0;32m    116\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid format specifier ' \"Bearer YOUR_API_KEY\"' for object of type 'str'"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "## CHAIN-OF-THOUGHT PROMPTING: PROJECT OVERVIEW + DATA & METHODOLOGIES\n",
    "############################################################\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "from _pipeline import create_payload, model_req\n",
    "\n",
    "# 1) Read the last response from the few-shot output CSV file\n",
    "with open(\"data/few_shot_logs.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    few_shot_output = list(reader)[-1][5]  # Assuming the response is in the sixth column\n",
    "\n",
    "# 2) Define a more detailed Chain-of-Thought prompt\n",
    "COT_PROMPT = f\"\"\"\n",
    "You are an AI that generates a comprehensive project report using chain-of-thought reasoning.\n",
    "First, reason internally about the best way to describe:\n",
    "1) Project Overview (Title, Goal, Problem Statement, Key Objectives, Scope)\n",
    "2) Data & Methodologies (Data Sources, Data Processing, Methodology, Approach)\n",
    "\n",
    "Then, provide a detailed final answer in Markdown:\n",
    "- Use multiple paragraphs and headings.\n",
    "- Expand on each point with descriptive explanations.\n",
    "- Use subheadings if necessary to organize information.\n",
    "\n",
    "[START OF INSTRUCTIONS]\n",
    "{few_shot_output}\n",
    "[END OF INSTRUCTIONS]\n",
    "\"\"\"\n",
    "\n",
    "# 3) Create a payload with moderate token usage\n",
    "payload_cot = create_payload(\n",
    "    target=\"open-webui\",\n",
    "    model=\"Llama-3.2-3B-Instruct\",  # Updated model\n",
    "    prompt=COT_PROMPT,\n",
    "    temperature=0.8,                # Slightly higher to encourage more detailed text\n",
    "    num_ctx=300,                    # Increase context for longer expansions\n",
    "    num_predict=400                 # Enough tokens for detailed paragraphs\n",
    ")\n",
    "\n",
    "def request_with_retry(payload, max_retries=2, delay=3):\n",
    "    \"\"\"\n",
    "    Attempts the model_req call up to `max_retries` times,\n",
    "    waiting `delay` seconds between tries if a 504 error occurs.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            time_cot, cot_output = model_req(payload=payload)\n",
    "            return time_cot, cot_output\n",
    "        except Exception as e:\n",
    "            error_str = str(e)\n",
    "            if \"504\" in error_str or \"Bad Gateway\" in error_str:\n",
    "                print(f\"Got 504 error. Retrying in {delay} seconds...\")\n",
    "                attempt += 1\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                # Different error; re-raise\n",
    "                raise e\n",
    "    raise RuntimeError(\"Max retries reached. 504 error persists.\")\n",
    "\n",
    "# 4) Execute the chain-of-thought request with retries\n",
    "time_cot, cot_output = request_with_retry(payload_cot)\n",
    "\n",
    "# 5) Print the final Markdown output\n",
    "print(\"===== CHAIN-OF-THOUGHT OUTPUT (PROJECT OVERVIEW + DATA & METHODOLOGIES) =====\")\n",
    "print(cot_output)\n",
    "if time_cot:\n",
    "    print(f\"\\nTime taken: {time_cot}s\")\n",
    "\n",
    "# 6) Save the chain-of-thought response to a variable and a CSV file for future reference\n",
    "with open(\"data/chain_of_thought_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(cot_output)\n",
    "\n",
    "log_entry = [\n",
    "    datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"chain_of_thought\",\n",
    "    \"Llama-3.2-3B-Instruct\",\n",
    "    0.8,\n",
    "    time_cot,\n",
    "    cot_output.replace(\"\\n\", \"\\\\n\")\n",
    "]\n",
    "\n",
    "with open(\"data/chain_of_thought_data_methods_logs.csv\", \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(log_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
